{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgq4z0TRsUOnNtUkjqVtIR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yahya94812/JAX-Tutorial/blob/main/JAX_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction To JAX**\n",
        "This notebook provide comprehensive introduction to JAX basics through Python code examples. JAX is a high-performance numerical computing library that combines NumPy's familiar API with the benefits of automatic differentiation and accelerated hardware like GPUs and TPUs.\n",
        "\n",
        "---\n",
        "## Table of content :\n",
        "1. Basic JAX Operations\n",
        "2. Automatic Differentiation\n",
        "3. Just-In-Time Compilation (jit)\n",
        "4. Vectorized Operations (vmap)\n",
        "5. Combining Transformations\n",
        "6. Gradient-Based Optimization"
      ],
      "metadata": {
        "id": "qHTaGqkHX46Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Basic JAX Operations :\n",
        "Shows how to create and manipulate JAX arrays, which are similar to NumPy arrays but designed for acceleration and transformation."
      ],
      "metadata": {
        "id": "pYkFKCWIJs9c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul0mkh9SHOJ8"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating JAX array\n",
        "x = jnp.array([2,4,6,8,10])\n",
        "y = jnp.ones(5)\n",
        "print(f\"jax array: {x}\")\n",
        "print(f\"jax onse: {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlbhkddJINAl",
        "outputId": "a15734bc-5055-44db-c660-e5c3ad6fb5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jax array: [ 2  4  6  8 10]\n",
            "jax onse: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic operations\n",
        "print(\"Addition:\", x + y)\n",
        "print(\"Multiplication:\", x * y)\n",
        "print(\"Dot product:\", jnp.dot(x, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT37AesgIzIF",
        "outputId": "92c3726d-51e3-4edb-f623-1343c6dbc216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Addition: [ 3.  5.  7.  9. 11.]\n",
            "Multiplication: [ 2.  4.  6.  8. 10.]\n",
            "Dot product: 30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX vs NumPy\n",
        "numpy_array = np.array([1, 2, 3, 4])\n",
        "jax_array = jnp.array([1, 2, 3, 4])\n",
        "\n",
        "print(\"NumPy array type:\", type(numpy_array))\n",
        "print(\"JAX array type:\", type(jax_array))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zbcpCC7JPJ7",
        "outputId": "a0f6e69b-7393-4c51-9e44-bf764254c919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy array type: <class 'numpy.ndarray'>\n",
            "JAX array type: <class 'jaxlib.xla_extension.ArrayImpl'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Automatic Differentiation :\n",
        " Demonstrates JAX's powerful automatic differentiation capabilities using the grad function, which computes gradients of functions automatically."
      ],
      "metadata": {
        "id": "5JPO1QFDJWDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple function\n",
        "def square(x):\n",
        "    return x ** 2\n",
        "\n",
        "# Compute the gradient of the function\n",
        "square_grad = grad(square)\n",
        "\n",
        "# Evaluate the gradient at x=3\n",
        "x_value = 3.0\n",
        "grad_value = square_grad(x_value)\n",
        "\n",
        "print(f\"Function: f(x) = x^2\")\n",
        "print(f\"Derivative: f'(x) = 2x\")\n",
        "print(f\"Gradient at x={x_value}: {grad_value}\")\n",
        "print(f\"Expected gradient: {2 * x_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M_o4cViJUsv",
        "outputId": "ef0a5d3f-8256-4c32-87af-bbe578190cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function: f(x) = x^2\n",
            "Derivative: f'(x) = 2x\n",
            "Gradient at x=3.0: 6.0\n",
            "Expected gradient: 6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A more complex function\n",
        "def tanh(x):\n",
        "    return (jnp.exp(x) - jnp.exp(-x)) / (jnp.exp(x) + jnp.exp(-x))\n",
        "\n",
        "# Compute the gradient of tanh\n",
        "tanh_grad = grad(tanh)\n",
        "\n",
        "# Evaluate the gradient at x=2.0\n",
        "x_value = 2.0\n",
        "grad_value = tanh_grad(x_value)\n",
        "\n",
        "print(f\"\\nFunction: tanh(x)\")\n",
        "print(f\"Gradient at x={x_value}: {grad_value}\")\n",
        "print(f\"Expected gradient: {1 - tanh(x_value)**2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCSHq_PFKgqA",
        "outputId": "302be3c4-d5ce-4ffe-90cc-904870b93f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Function: tanh(x)\n",
            "Gradient at x=2.0: 0.07065093517303467\n",
            "Expected gradient: 0.07065081596374512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Just-In-Time Compilation (jit) :\n",
        "Shows how to use JAX's JIT compiler to significantly speed up function execution."
      ],
      "metadata": {
        "id": "6ocsWwwJOsX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function\n",
        "def slow_function(x):\n",
        "    # Simulating a complex calculation\n",
        "    for _ in range(10000):\n",
        "        x = x + 1e-7\n",
        "    return x\n",
        "\n",
        "# Create a JIT-compiled version\n",
        "fast_function = jit(slow_function)"
      ],
      "metadata": {
        "id": "Z_Qeca8NOruw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure execution time\n",
        "\n",
        "x_value = 5.0\n",
        "\n",
        "# Warm-up\n",
        "#  When we use jit in JAX, the first time you call a function,\n",
        "# -JAX actually compiles it for the specific input shapes.\n",
        "_ = slow_function(x_value)\n",
        "_ = fast_function(x_value)\n",
        "\n",
        "# Time the slow function\n",
        "start_time = time.time()\n",
        "result_slow = slow_function(x_value)\n",
        "slow_time = time.time() - start_time\n",
        "\n",
        "# Time the fast function\n",
        "start_time = time.time()\n",
        "result_fast = fast_function(x_value)\n",
        "fast_time = time.time() - start_time\n",
        "\n",
        "print(f\"Slow function result: {result_slow}\")\n",
        "print(f\"Fast function result: {result_fast}\")\n",
        "print(f\"Slow function time: {slow_time:.6f} seconds\")\n",
        "print(f\"Fast function time: {fast_time:.6f} seconds\")\n",
        "print(f\"Speedup: {slow_time / fast_time:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RzMFwIAREVz",
        "outputId": "a2faece2-f3c1-460f-9f15-f4e7b245fb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slow function result: 5.001000000002804\n",
            "Fast function result: 5.000999927520752\n",
            "Slow function time: 0.000373 seconds\n",
            "Fast function time: 0.000371 seconds\n",
            "Speedup: 1.01x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Vectorized Operations with vmap :\n",
        " Illustrates how to vectorize functions with vmap, allowing them to operate efficiently on batches of inputs."
      ],
      "metadata": {
        "id": "yBL-7r3tRYzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that operates on scalars\n",
        "def scalar_function(x, y):\n",
        "    return x * y + jnp.sin(x)\n",
        "\n",
        "# Create a vectorized version\n",
        "vector_function = vmap(scalar_function, in_axes=(0, None))\n",
        "# Vectorizes over the first argument (x) as indicated by 0 in in_axes\n",
        "# Keeps the second argument (y) as a scalar, indicated by None in in_axes\n",
        "\n",
        "# Apply to a vector and a scalar\n",
        "x_values = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
        "y_value = 2.0\n",
        "\n",
        "result = vector_function(x_values, y_value)\n",
        "# it is similar to [scalar_function(1.0, 2.0), scalar_function(2.0, 2.0), scalar_function(3.0, 2.0), scalar_function(4.0, 2.0)]\n",
        "print(f\"Input vector: {x_values}\")\n",
        "print(f\"Scalar: {y_value}\")\n",
        "print(f\"Vectorized result: {result}\")\n",
        "\n",
        "# Manual calculation for comparison\n",
        "manual_result = jnp.array([scalar_function(x, y_value) for x in x_values])\n",
        "print(f\"Manual result: {manual_result}\")\n",
        "print(f\"Results match: {jnp.allclose(result, manual_result)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgH2ops_Ra0x",
        "outputId": "48e825e1-ada7-48d4-bab6-ac443258538f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input vector: [1. 2. 3. 4.]\n",
            "Scalar: 2.0\n",
            "Vectorized result: [2.841471  4.9092975 6.14112   7.2431974]\n",
            "Manual result: [2.841471  4.9092975 6.14112   7.2431974]\n",
            "Results match: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Combining JAX Transformations :\n",
        "Demonstrates how to combine JAX transformations like jit and vmap for maximum performance."
      ],
      "metadata": {
        "id": "DPtB2nTaVXkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for a simple neural network layer\n",
        "def layer(params, x):\n",
        "    w, b = params\n",
        "    return jnp.dot(x, w) + b\n",
        "\n",
        "# Create a batch version that operates on multiple inputs\n",
        "batch_layer = vmap(layer, in_axes=(None, 0))\n",
        "\n",
        "# JIT-compile the batch version\n",
        "fast_batch_layer = jit(batch_layer)\n",
        "\n",
        "# Create some test data\n",
        "batch_size = 256\n",
        "input_dim = 128\n",
        "output_dim = 64\n",
        "\n",
        "W = jnp.ones((input_dim, output_dim))\n",
        "b = jnp.zeros(output_dim)\n",
        "params = (W, b)\n",
        "inputs = jnp.ones((batch_size, input_dim))\n",
        "\n",
        "# Warm-up\n",
        "_ = fast_batch_layer(params, inputs)\n",
        "\n",
        "# Time the execution\n",
        "start_time = time.time()\n",
        "result = fast_batch_layer(params, inputs)\n",
        "batch_time = time.time() - start_time\n",
        "\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Input dimension: {input_dim}\")\n",
        "print(f\"Output dimension: {output_dim}\")\n",
        "print(f\"Output shape: {result.shape}\")\n",
        "print(f\"Execution time: {batch_time:.6f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_EoK4JgVbwd",
        "outputId": "2a649fb6-77e7-4454-94dc-f1c486931703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 256\n",
            "Input dimension: 128\n",
            "Output dimension: 64\n",
            "Output shape: (256, 64)\n",
            "Execution time: 0.000320 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Gradient-Based Optimization :\n",
        "Shows a simple gradient descent optimization example using JAX's automatic differentiation"
      ],
      "metadata": {
        "id": "j6jrLjUPWHUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple loss function\n",
        "def loss(params, x, y):\n",
        "    pred = jnp.dot(x, params)\n",
        "    return jnp.mean((pred - y) ** 2)\n",
        "\n",
        "# Create gradient function\n",
        "loss_grad = jit(grad(loss))\n",
        "\n",
        "# Generate some random data\n",
        "key = jax.random.PRNGKey(42)\n",
        "x_data = jax.random.normal(key, (100, 5))\n",
        "true_params = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "y_data = jnp.dot(x_data, true_params) + jax.random.normal(jax.random.PRNGKey(0), (100,)) * 0.1\n",
        "\n",
        "# Initial parameters\n",
        "params = jnp.zeros(5)\n",
        "\n",
        "# Perform gradient descent\n",
        "n_steps = 100\n",
        "step_size = 0.1\n",
        "\n",
        "print(f\"Initial loss: {loss(params, x_data, y_data):.4f}\")\n",
        "\n",
        "for i in range(n_steps):\n",
        "    grads = loss_grad(params, x_data, y_data)\n",
        "    params = params - step_size * grads\n",
        "\n",
        "    if i % 20 == 0 or i == n_steps - 1:\n",
        "        current_loss = loss(params, x_data, y_data)\n",
        "        print(f\"Step {i}, Loss: {current_loss:.4f}\")\n",
        "\n",
        "print(f\"True parameters: {true_params}\")\n",
        "print(f\"Learned parameters: {params}\")"
      ],
      "metadata": {
        "id": "pmvQFu2zWJCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ecdea7-5942-4958-dc7d-d4eac22ea7ad"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial loss: 49.8177\n",
            "Step 0, Loss: 33.2453\n",
            "Step 20, Loss: 0.0267\n",
            "Step 40, Loss: 0.0085\n",
            "Step 60, Loss: 0.0085\n",
            "Step 80, Loss: 0.0085\n",
            "Step 99, Loss: 0.0085\n",
            "True parameters: [1. 2. 3. 4. 5.]\n",
            "Learned parameters: [1.008449  1.9874883 2.9950392 4.0158367 4.9924345]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Thank You**"
      ],
      "metadata": {
        "id": "4ovYT65-airQ"
      }
    }
  ]
}